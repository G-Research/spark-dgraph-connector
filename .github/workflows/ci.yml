name: CI

on:
  push:
    branches:
      - 'spark-*'
    tags:
      - '*'
  pull_request:

jobs:
  build:
    name: Build
    runs-on: ubuntu-latest
    outputs:
      spark-version: ${{ steps.params.outputs.spark-version }}
      spark-compat-version: ${{ steps.params.outputs.spark-compat-version }}
      spark-version-matrix: ${{ format('["{0}"]', steps.params.outputs.spark-version) }}

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Parametrize Extraction
        id: params-extract
        run: |
          echo "::set-output name=spark-version::$(grep --max-count=1 "<spark.version>.*</spark.version>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")"
          echo "::set-output name=spark-compat-version::$(grep --max-count=1 "<spark.compat.version>.*</spark.compat.version>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")"
        shell: bash
      - name: Parametrize
        id: params
        env:
          SPARK_VERSION: ${{ steps.params-extract.outputs.spark-version }}
          SPARK_COMPAT_VERSION: ${{ steps.params-extract.outputs.spark-compat-version }}
        run: |
          echo "::set-output name=spark-version::$(echo "$SPARK_VERSION" | sed "s/\${spark.compat.version}/$SPARK_COMPAT_VERSION/")"
          echo "::set-output name=spark-compat-version::$SPARK_COMPAT_VERSION"
        shell: bash

      - name: Cache Maven packages
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-mvn-build-${{ steps.params.outputs.spark-version }}-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-mvn-build-

      - name: Setup JDK 1.8
        uses: actions/setup-java@v1
        with:
          java-version: 1.8

      - name: Build
        run: mvn --batch-mode compile test-compile package -DskipTests

      - name: Upload Binaries
        uses: actions/upload-artifact@v2
        with:
          name: Binaries
          path: |
            target
            !target/*-javadoc.jar
            !target/site

  test:
    name: Unit Tests (Dgraph ${{ matrix.dgraph-version }} and Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    needs: build

    strategy:
      fail-fast: false
      matrix:
        dgraph-version: [20.03.4, 20.03.5, 20.03.6, 20.03.7, 20.07.0, 20.07.1, 20.07.2, 20.11.0, 20.11.1, 20.11.2]
        python-version: [3.6]

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - name: Fetch Binaries Artifact
      uses: actions/download-artifact@v2
      with:
        name: Binaries
        path: target

    - name: Cache Maven packages
      uses: actions/cache@v2
      with:
        path: ~/.m2/repository
        key: ${{ runner.os }}-mvn-test-${{ steps.params.outputs.spark-version }}-${{ hashFiles('**/pom.xml') }}
        restore-keys: |
          ${{ runner.os }}-mvn-test-

    - name: Setup JDK 1.8
      uses: actions/setup-java@v1
      with:
        java-version: 1.8

    - name: Scala Test
      env:
        DGRAPH_TEST_CLUSTER_VERSION: ${{ matrix.dgraph-version }}
      run: mvn --batch-mode test

    - name: Cache Pip packages
      uses: actions/cache@v2
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-test-${{ matrix.python-version }}-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-test-${{ matrix.python-version }}-

    - name: Setup Python ${{ matrix.python-version }}
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r python/requirements.txt
        pip install pytest

    - name: Python Test
      env:
        DGRAPH_TEST_CLUSTER_VERSION: ${{ matrix.dgraph-version }}
        PYTHONPATH: python:python/test
      run: |
        mkdir -p target/surefire-reports
        python -m pytest python/test --junit-xml target/surefire-reports/pytest.xml

    - name: Generate Unit Test Report
      if: failure()
      run: mvn --batch-mode surefire-report:report-only

    - name: Upload Unit Test Results
      if: always()
      uses: actions/upload-artifact@v2
      with:
        name: Unit Test Results (Dgraph ${{ matrix.dgraph-version }} and Python ${{ matrix.python-version }})
        path: |
          target/surefire-reports/*.xml
          !target/surefire-reports/TEST-org.scalatest*.xml
          target/site/surefire-report.html

  publish-test-results:
    name: Publish Unit Tests Results
    needs: test

    runs-on: ubuntu-latest
    # the workflow is useless on pull_request events from fork repositories
    # as it can not create check runs or pull request comments
    if: >
      ( success() || failure() ) &&
      ( github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository )

    steps:
      - name: Download Artifacts
        uses: actions/download-artifact@v2
        with:
          path: artifacts

      - name: Publish Unit Test Results
        uses: docker://ghcr.io/enricomi/publish-unit-test-result-action:v1
        with:
          github_token: ${{ github.token }}
          files: "artifacts/Unit Test Results*/**/*.xml"
          comment_on_pr: ${{ github.event_name != 'push' }}
          log_level: DEBUG

  integration:
    name: Integration Tests (Dgraph ${{ matrix.dgraph-version }})
    runs-on: ubuntu-latest
    needs: build
    if: needs.build.outputs.spark-compat-version != '3.1'

    strategy:
      fail-fast: false
      matrix:
        spark-version: ${{ fromJSON(needs.build.outputs.spark-version-matrix) }}
        dgraph-version: [20.03.7, 20.07.2, 20.11.2]
        include:
          - spark-version: ${{ needs.build.outputs.spark-version }}
            spark-compat-version: ${{ needs.build.outputs.spark-compat-version }}

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - name: Fetch Binaries Artifact
      uses: actions/download-artifact@v2
      with:
        name: Binaries
        path: target

    - name: Cache Maven packages
      uses: actions/cache@v2
      with:
        path: ~/.m2/repository
        key: ${{ runner.os }}-mvn-integration-${{ matrix.spark-version }}-${{ hashFiles('**/pom.xml') }}
        restore-keys: |
          ${{ runner.os }}-mvn-integration-

    - name: Cache Spark Binaries
      uses: actions/cache@v2
      with:
        path: ~/spark
        key: ${{ runner.os }}-spark-binaries-${{ matrix.spark-version }}

    - name: Setup Spark Binaries
      env:
        SPARK_PACKAGE: spark-${{ matrix.spark-version }}/spark-${{ matrix.spark-version }}-bin-hadoop2.7.tgz
      run: |
        if [[ ! -e ~/spark ]]
        then
          wget --progress=dot:giga https://archive.apache.org/dist/spark/${SPARK_PACKAGE} -O - | tar -xzC "${{ runner.temp }}"
          archive=$(basename "${SPARK_PACKAGE}") bash -c "mv -v "${{ runner.temp }}/\${archive/%.tgz/}" ~/spark"
        fi
      shell: bash

    - name: Parametrize
      id: params
      run: |
        echo "::set-output name=artifact-id::$(grep --max-count=1 "<artifactId>.*</artifactId>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")"
        echo "::set-output name=version::$(grep --max-count=1 "<version>.*</version>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")"
        echo "::set-output name=graphframes-version::$(grep -A 1 "<artifactId>graphframes</artifactId>" pom.xml | tail -n 1 | sed -E -e "s/^ +<version>//" -e "s/-.*//")"
        echo "::set-output name=home::$(cd ~; pwd)"
      shell: bash

    - name: Prepare Integration Tests
      run: |
        mvn --batch-mode install -DskipTests
        cd examples/scala
        mvn --batch-mode package
        # spark-submit is not capable of downloading these dependencies, fetching them through mvn
        mvn --batch-mode dependency:get -DgroupId=com.google.errorprone -DartifactId=error_prone_annotations -Dversion=2.3.3
        mvn --batch-mode dependency:get -DgroupId=com.google.code.findbugs -DartifactId=jsr305 -Dversion=3.0.2
        mvn --batch-mode dependency:get -DgroupId=org.codehaus.mojo -DartifactId=animal-sniffer-annotations -Dversion=1.17
        mvn --batch-mode dependency:get -DgroupId=com.google.code.gson -DartifactId=gson -Dversion=2.8.6
        mvn --batch-mode dependency:get -DgroupId=org.slf4j -DartifactId=slf4j-api -Dversion=1.7.16
      shell: bash

    - name: Start Dgraph cluster
      id: dgraph
      env:
        DGRAPH_TEST_CLUSTER_VERSION: ${{ matrix.dgraph-version }}
      run: |
        echo "::set-output name=docker::$(./dgraph-instance.background.sh)"
        sleep 10
        if [[ "${{ matrix.dgraph-version }}" != "20.03."* ]]
        then
          ./dgraph-instance.drop-all.sh
        fi
        ./dgraph-instance.schema.sh
        ./dgraph-instance.insert.sh
      shell: bash

    - name: Integration Tests
      env:
        SPARK_HOME: ${{ steps.params.outputs.home }}/spark
        ARTIFACT_ID: ${{ steps.params.outputs.artifact-id }}
        VERSION: ${{ steps.params.outputs.version }}
      run: |
        ${SPARK_HOME}/bin/spark-submit --packages uk.co.gresearch.spark:${ARTIFACT_ID}:${VERSION},graphframes:graphframes:${{ steps.params.outputs.graphframes-version }}-spark${{ matrix.spark-compat-version }}-s_2.12 --class uk.co.gresearch.spark.dgraph.connector.example.ExampleApp examples/scala/target/spark-dgraph-connector-examples_*.jar
      shell: bash

    - name: Stop Dgraph cluster
      if: always() && steps.dgraph.outcome == 'success'
      run: |
        docker stop ${{ steps.dgraph.outputs.docker }}
      shell: bash
