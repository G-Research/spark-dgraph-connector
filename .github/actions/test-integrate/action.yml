name: 'Test Integration'
author: 'EnricoMi'
description: 'A GitHub Action that tests spark-dgraph-connector integrating with Dgraph and Spark'

inputs:
  spark-version:
    description: Spark version, e.g. 3.4.0 or 3.4.0-SNAPSHOT
    required: true
  scala-version:
    description: Scala version, e.g. 2.12.15
    required: true
  spark-compat-version:
    description: Spark compatibility version, e.g. 3.4
    required: true
  scala-compat-version:
    description: Scala compatibility version, e.g. 2.12
    required: true
  dgraph-version:
    description: Dgraph server version, e.g. 22.0.0
    required: true
  hadoop-version:
    description: Hadoop version, e.g. 2.7 or 2
    required: true

runs:
  using: 'composite'
  steps:
    - name: Set versions in pom.xml
      run: |
        ./set-version.sh ${{ inputs.spark-version }} ${{ inputs.scala-version }}
        git diff
      shell: bash

    - name: Fetch Binaries Artifact
      uses: actions/download-artifact@v3
      with:
        name: Binaries-${{ inputs.spark-compat-version }}-${{ inputs.scala-compat-version }}
        path: .

    - name: Cache Maven packages
      uses: actions/cache@v3
      with:
        path: ~/.m2/repository
        key: ${{ runner.os }}-mvn-integrate-${{ inputs.spark-version }}-${{ inputs.scala-compat-version }}-${{ hashFiles('**/pom.xml') }}
        restore-keys: |
          ${{ runner.os }}-mvn-integrate-${{ inputs.spark-version }}-${{ inputs.scala-compat-version }}-
          ${{ runner.os }}-mvn-build-${{ inputs.spark-version }}-${{ inputs.scala-compat-version }}-

    - name: Cache Spark Binaries
      uses: actions/cache@v3
      with:
        path: ~/spark
        key: ${{ runner.os }}-spark-binaries-${{ inputs.spark-version }}-${{ inputs.scala-compat-version }}

    - name: Setup Spark Binaries
      env:
        SPARK_PACKAGE: spark-${{ inputs.spark-version }}/spark-${{ inputs.spark-version }}-bin-hadoop${{ inputs.hadoop-version }}.tgz
      run: |
        if [[ ! -e ~/spark ]]
        then
          wget --progress=dot:giga "https://www.apache.org/dyn/closer.lua/spark/${SPARK_PACKAGE}?action=download" -O - | tar -xzC "${{ runner.temp }}"
          archive=$(basename "${SPARK_PACKAGE}") bash -c "mv -v "${{ runner.temp }}/\${archive/%.tgz/}" ~/spark"
        fi
      shell: bash

    - name: Parametrize
      id: params
      run: |
        echo "artifact-id=$(grep --max-count=1 "<artifactId>.*</artifactId>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")" >> $GITHUB_OUTPUT
        echo "version=$(grep --max-count=1 "<version>.*</version>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")" >> $GITHUB_OUTPUT
        echo "graphframes-version=$(grep --max-count=1 "<graphframes.version>.*</graphframes.version>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")" >> $GITHUB_OUTPUT
        echo "home=$(cd ~; pwd)" >> $GITHUB_OUTPUT
      shell: bash

    - name: Prepare Integration Tests
      run: |
        mvn --batch-mode install -DskipTests
        cd examples/scala
        mvn --batch-mode package
        # spark-submit is not capable of downloading these dependencies, fetching them through mvn
        mvn --batch-mode dependency:get -DgroupId=com.google.errorprone -DartifactId=error_prone_annotations -Dversion=2.3.3
        mvn --batch-mode dependency:get -DgroupId=com.google.code.findbugs -DartifactId=jsr305 -Dversion=3.0.2
        mvn --batch-mode dependency:get -DgroupId=org.codehaus.mojo -DartifactId=animal-sniffer-annotations -Dversion=1.17
        mvn --batch-mode dependency:get -DgroupId=com.google.code.gson -DartifactId=gson -Dversion=2.8.9
        mvn --batch-mode dependency:get -DgroupId=org.slf4j -DartifactId=slf4j-api -Dversion=1.7.16
      shell: bash

    - name: Start Dgraph cluster (Small)
      id: dgraph-small
      env:
        DGRAPH_TEST_CLUSTER_VERSION: ${{ inputs.dgraph-version }}
      run: |
        cp -v dgraph-instance.*.sh /tmp/
        echo "docker=$(/tmp/dgraph-instance.background.sh)" >> $GITHUB_OUTPUT
        sleep 10
        if [[ "${{ inputs.dgraph-version }}" != "20.03."* ]]
        then
          /tmp/dgraph-instance.drop-all.sh
        fi
        /tmp/dgraph-instance.schema.sh
        /tmp/dgraph-instance.insert.sh
      shell: bash

    - name: Integration Test (Example)
      env:
        SPARK_LOCAL_IP: 127.0.0.1
        SPARK_HOME: ${{ steps.params.outputs.home }}/spark
        ARTIFACT_ID: ${{ steps.params.outputs.artifact-id }}
        VERSION: ${{ steps.params.outputs.version }}
      # There is only a Spark 3.0 release of graphframes
      run: |
        ${SPARK_HOME}/bin/spark-submit --packages uk.co.gresearch.spark:${ARTIFACT_ID}:${VERSION},graphframes:graphframes:${{ steps.params.outputs.graphframes-version }}-s_${{ inputs.scala-compat-version }},org.scalactic:scalactic_${{ inputs.scala-compat-version }}:3.2.15 --class uk.co.gresearch.spark.dgraph.connector.example.ExampleApp examples/scala/target/spark-dgraph-connector-examples_*.jar
      shell: bash

    - name: Stop Dgraph cluster
      run: docker stop ${{ steps.dgraph-small.outputs.docker }}
      shell: bash

    - name: Start Dgraph cluster (Large)
      id: dgraph
      run: |
        if [[ $dgraph < "v21.03.0" ]]
        then
          cache=""
          whitelist="--whitelist=0.0.0.0/0"
          maxUID="maxLeaseId"
        else
          cache="--cache size-mb=2048"
          whitelist="--security whitelist=0.0.0.0/0"
          maxUID="maxUID"
        fi

        mkdir dgraph
        curl -L -o dgraph/1million.rdf.gz "https://github.com/dgraph-io/tutorial/blob/master/resources/1million.rdf.gz?raw=true"
        docker run --rm -p 5080:5080 -p 6080:6080 -p 8080:8080 -p 9080:9080 -v $(pwd)/dgraph:/dgraph --name dgraph dgraph/dgraph:v${{ inputs.dgraph-version }} dgraph zero &
        sleep 2
        docker exec dgraph dgraph alpha $cache --zero localhost:5080 $whitelist &

        for attempt in {1..10}
        do
          sleep 10
          echo "attempt $attempt"
          if curl --data-binary @dgraph-instance.schema.live-loader.dql -H "Content-Type: text/plain;charset=UTF-8" http://localhost:8080/alter; then break; fi
          if [ $attempt -eq 10 ]; then exit 1; fi
        done

        docker exec dgraph dgraph live -f 1million.rdf.gz --alpha localhost:9080 --zero localhost:5080 -c 1
      shell: bash

    - name: Integration Test (Sparse)
      env:
        SPARK_LOCAL_IP: 127.0.0.1
        SPARK_HOME: ${{ steps.params.outputs.home }}/spark
        ARTIFACT_ID: ${{ steps.params.outputs.artifact-id }}
        VERSION: ${{ steps.params.outputs.version }}
      run: |
        ${SPARK_HOME}/bin/spark-submit --packages uk.co.gresearch.spark:${ARTIFACT_ID}:${VERSION},org.scalactic:scalactic_2.12:3.2.15 --class uk.co.gresearch.spark.dgraph.connector.example.SparseApp examples/scala/target/spark-dgraph-connector-examples_*.jar
      shell: bash

branding:
  icon: 'check-circle'
  color: 'green'
