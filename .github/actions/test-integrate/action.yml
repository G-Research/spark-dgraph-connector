name: 'Test Integration'
author: 'EnricoMi'
description: 'A GitHub Action that tests spark-dgraph-connector integrating with Dgraph and Spark'

inputs:
  spark-version:
    description: Spark version, e.g. 3.4.0 or 3.4.0-SNAPSHOT
    required: true
  scala-version:
    description: Scala version, e.g. 2.12.15
    required: true
  spark-compat-version:
    description: Spark compatibility version, e.g. 3.4
    required: true
  scala-compat-version:
    description: Scala compatibility version, e.g. 2.12
    required: true
  java-version:
    description: Java version, e.g. 11
    required: true
  dgraph-version:
    description: Dgraph server version, e.g. 22.0.0
    required: true
  hadoop-version:
    description: Hadoop version, e.g. 2.7 or 2
    required: true

runs:
  using: 'composite'
  steps:
    - name: Set versions in pom.xml
      run: |
        ./set-version.sh ${{ inputs.spark-version }} ${{ inputs.scala-version }}
        git diff
      shell: bash

    - name: Fetch Binaries Artifact
      uses: actions/download-artifact@v4
      with:
        name: Binaries-${{ inputs.spark-compat-version }}-${{ inputs.scala-compat-version }}
        path: .

    - name: Fetch Dependencies Artifact
      uses: actions/download-artifact@v4
      with:
        name: Dependencies-${{ inputs.spark-compat-version }}-${{ inputs.scala-compat-version }}
        path: ~/.m2/repository

    - name: Fetch Spark Binaries Artifact
      uses: actions/download-artifact@v4
      with:
        name: Spark-Binaries-${{ inputs.spark-version }}-${{ inputs.hadoop-version }}
        path: ~/spark

    - name: Change file permissions
      run: chmod u+x ~/spark/bin/* ~/spark/sbin/*
      shell: bash

    - name: Setup JDK
      uses: actions/setup-java@v4
      with:
        java-version: ${{ inputs.java-version }}
        distribution: 'zulu'

    - name: Parametrize
      id: params
      run: |
        echo "artifact-id=$(grep --max-count=1 "<artifactId>.*</artifactId>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")" >> $GITHUB_OUTPUT
        echo "version=$(grep --max-count=1 "<version>.*</version>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")" >> $GITHUB_OUTPUT
        echo "graphframes-version=$(grep --max-count=1 "<graphframes.version>.*</graphframes.version>" pom.xml | sed -E -e "s/\s*<[^>]+>//g")" >> $GITHUB_OUTPUT
        echo "home=$(cd ~; pwd)" >> $GITHUB_OUTPUT
      shell: bash

    - name: Prepare Integration Tests
      run: |
        mvn --batch-mode -Dspotless.check.skip -DskipTests install
        (cd examples/scala && mvn --batch-mode -Dspotless.check.skip package)
        # spark-submit is not capable of downloading these dependencies, fetching them through mvn
        for dep in "org.slf4j#slf4j-api;2.0.16" \
                   "com.google.protobuf#protobuf-java;4.29.1" \
                   "io.netty#netty-all;4.1.110.Final" \
                   "com.google.guava#guava;33.3.1-jre" \
                   "com.google.guava#failureaccess;1.0.2" \
                   "com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava" \
                   "org.checkerframework#checker-qual;3.43.0" \
                   "com.google.j2objc#j2objc-annotations;3.0.0"; do
          IFS="#;" read group artifact version <<< "$dep"
          mvn --batch-mode -Dspotless.check.skip dependency:get -DgroupId="$group" -DartifactId="$artifact" -Dversion="$version"
        done
        if [[ "${{ inputs.spark-compat-version }}" == "3.0" ]]
        then
          # spark-submit 3.0 cannot resolve the dgraph4j dependency that has classifier "shaded"
          # copying it into .ivy2 cache without classifier
          mkdir -p ~/.ivy2/jars/
          dgraph4j_version="$(grep -A3 -B2 "<artifactId>dgraph4j</artifactId>" pom.xml | grep "<version>" | sed -e "s/[^>]*>//" -e "s/<.*//")"
          cp -v ~/.m2/repository/io/dgraph/dgraph4j/${dgraph4j_version}/dgraph4j-${dgraph4j_version}-shaded.jar ~/.ivy2/jars/io.dgraph_dgraph4j-${dgraph4j_version}.jar
        fi
      shell: bash

    - name: Start Dgraph cluster (Small)
      id: dgraph-small
      env:
        DGRAPH_TEST_CLUSTER_VERSION: ${{ inputs.dgraph-version }}
      run: |
        cp -v dgraph-instance.*.sh /tmp/
        echo "docker=$(/tmp/dgraph-instance.background.sh)" >> $GITHUB_OUTPUT
        sleep 10
        if [[ "${{ inputs.dgraph-version }}" != "20.03."* ]]
        then
          /tmp/dgraph-instance.drop-all.sh
        fi
        /tmp/dgraph-instance.schema.sh
        /tmp/dgraph-instance.insert.sh
      shell: bash

    - name: Integration Test (Example)
      env:
        SPARK_LOCAL_IP: 127.0.0.1
        SPARK_HOME: ${{ steps.params.outputs.home }}/spark
        ARTIFACT_ID: ${{ steps.params.outputs.artifact-id }}
        VERSION: ${{ steps.params.outputs.version }}
      run: |
        ${SPARK_HOME}/bin/spark-submit --packages uk.co.gresearch.spark:${ARTIFACT_ID}:${VERSION},graphframes:graphframes:${{ steps.params.outputs.graphframes-version }}-s_${{ inputs.scala-compat-version }},org.scalactic:scalactic_${{ inputs.scala-compat-version }}:3.2.15 --class uk.co.gresearch.spark.dgraph.connector.example.ExampleApp examples/scala/target/spark-dgraph-connector-examples_*.jar
      shell: bash

    - name: Stop Dgraph cluster
      if: inputs.dgraph-version != '21.12.0'
      run: docker stop ${{ steps.dgraph-small.outputs.docker }}
      shell: bash

    - name: Start Dgraph cluster (Large)
      id: dgraph
      if: inputs.dgraph-version != '21.12.0'
      run: |
        if [[ "${{ inputs.dgraph-version }}" < "21.03.0" ]]
        then
          cache=""
          whitelist="--whitelist=0.0.0.0/0"
          maxUID="maxLeaseId"
        else
          cache="--cache size-mb=2048"
          whitelist="--security whitelist=0.0.0.0/0"
          maxUID="maxUID"
        fi

        mkdir dgraph
        curl -L -o dgraph/1million.rdf.gz "https://github.com/dgraph-io/tutorial/blob/master/resources/1million.rdf.gz?raw=true"
        docker run --rm -p 5080:5080 -p 6080:6080 -p 8080:8080 -p 9080:9080 -v $(pwd)/dgraph:/dgraph --name dgraph dgraph/dgraph:v${{ inputs.dgraph-version }} dgraph zero &
        sleep 2
        docker exec dgraph dgraph alpha $cache --zero localhost:5080 $whitelist &

        for attempt in {1..10}
        do
          sleep 10
          echo "attempt $attempt"
          if curl --data-binary @dgraph-instance.schema.live-loader.dql -H "Content-Type: text/plain;charset=UTF-8" http://localhost:8080/alter; then break; fi
          if [ $attempt -eq 10 ]; then exit 1; fi
        done

        docker exec dgraph dgraph live -f 1million.rdf.gz --alpha localhost:9080 --zero localhost:5080 -c 1
      shell: bash

    - name: Integration Test (Sparse)
      if: inputs.dgraph-version != '21.12.0'
      env:
        SPARK_LOCAL_IP: 127.0.0.1
        SPARK_HOME: ${{ steps.params.outputs.home }}/spark
        ARTIFACT_ID: ${{ steps.params.outputs.artifact-id }}
        VERSION: ${{ steps.params.outputs.version }}
      run: |
        ${SPARK_HOME}/bin/spark-submit --packages uk.co.gresearch.spark:${ARTIFACT_ID}:${VERSION},org.scalactic:scalactic_2.12:3.2.15 --class uk.co.gresearch.spark.dgraph.connector.example.SparseApp examples/scala/target/spark-dgraph-connector-examples_*.jar
      shell: bash

branding:
  icon: 'check-circle'
  color: 'green'
